# ==============================================================================
# M3 CONFIGURATION FILE
# ==============================================================================
# This file defines the global settings for your m3 tool.
# A user-specific version will be created and managed at ~/.monkey3/config.yaml
# ==============================================================================

project_settings:
  # Defines the directory where all project data will be stored.
  # Using '~/.monkey3/projects' is the recommended default.
  projects_directory: '~/.monkey3/projects'

# ==============================================================================
# EMBEDDING MODEL SETTINGS
# ==============================================================================
embedding_settings:
  provider: 'ollama'
  # Change this line to your preferred model
  model_name: 'intfloat/multilingual-e5-large' # <--- YOUR PREFERRED MODEL HERE

# ==============================================================================
# LLM PROVIDER DEFINITIONS
# ==============================================================================
# Define the LLMs you want to use, served via Ollama.
llm_providers:
  ollama_client:
    provider: 'ollama'
    base_url: 'http://10.255.255.254:11434'
    #base_url: 'http://localhost:11434'
    models:
      # --- NEW MODEL ROLE ---
      # A model for very heavy, long-running tasks like Stage 0
      stratify_model:
        model_name: 'mistral'
        request_timeout: 300.0  # 5-minute timeout

      # A powerful model for complex reasoning (e.g., llama3, mixtral)
      synthesis_model:
        model_name: 'mistral'
        request_timeout: 120.0

      # A fast model for routine tasks (e.g., mistral, llama3:8b)
      enrichment_model:
        model_name: 'mistral'
        request_timeout: 60.0

# ==============================================================================
# INGESTION PIPELINE SETTINGS
# ==============================================================================
ingestion_config:
  # Pre-defined list of document types for the `corpus add --type` command
  known_doc_types:
    - 'document'
    - 'interview'
    - 'paper'
    - 'data'
    - 'observation'
    - 'ethnographic_notes'

  # The default document type if the --type flag is not used.
  default_doc_type: 'document'

  # Assigns the LLMs defined above to the specific stages of the pipeline.
  cogarc_settings:
    # --- MODIFIED ---
    # Assign the new, long-timeout model to Stage 0
    stage_0_model: 'stratify_model'
    # --- END MODIFIED ---

    stage_1_model: 'synthesis_model'
    stage_2_model: 'enrichment_model'
    stage_3_model: 'synthesis_model'

# ==============================================================================
# ANALYSIS SETTINGS
# ==============================================================================
analysis_settings:
  # --- "ALLOW LIST" ---
  # Defines which metadata keys should be EMBEDDED with the chunk content
  # to make them searchable in topk, search, and clustering.
  metadata_keys_to_embed:
    - 'themes'
    - 'hypothetical_question'
    - 'axial_theme'

  # --- "IGNORE LIST" ---
  # Defines which metadata keys to HIDE by default when displaying chunks.
  # These are identifiers or redundant fields not useful for display.
  metadata_keys_to_hide_display:
    - 'holistic_summary'
    - 'original_filename'
    - 'file_path'
    - 'original_text'
    - 'cluster_id'
    - 'file_type'
    - 'chunk_id'
    - 'chunk_total'
    - 'hash'